{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5c0ee6-895c-467d-9c3b-38dcef3e3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0314e1fe-196a-4991-86e4-9b7208412f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ObjectDetection.voc import make_filepath_list, GetBBoxAndLabel, DataTransform, PreprocessVOC2012, multiobject_collate_fn\n",
    "from ObjectDetection.ssd import SSD, MultiBoxLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2fbd6cc-55cb-4ab3-9aff-40432945a6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f77ec5-bc38-4242-8491-d5b1919d1292",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d57223-1c80-4b9a-a1ac-59f74450c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC2012の正解ラベルのリスト\n",
    "voc_classes = [\n",
    "    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
    "    'bus', 'car', 'cat', 'chair', 'cow',\n",
    "    'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train',\n",
    "    'tvmonitor']\n",
    "\n",
    "# データセットのGBR\n",
    "color_mean = (104, 117, 123)\n",
    "\n",
    "# 画像を入力サイズを300x300\n",
    "input_size = 300\n",
    "\n",
    "batch_size = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0295cd4b-193d-47dc-ae7e-c2bdc0cbb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_filepath_list(\n",
    "    \"./data/voc2012/VOC2012/\"\n",
    ")\n",
    "\n",
    "train_dataset = PreprocessVOC2012(\n",
    "    train_img_list,  # イメージのパスリスト\n",
    "    train_anno_list, # アノテーションのパスリスト\n",
    "    phase=\"train\",   # 訓練モード\n",
    "    transform=DataTransform(input_size, color_mean), # 前処理オブジェクト\n",
    "    get_bbox_label=GetBBoxAndLabel(voc_classes)) # BBoxとラベル取得\n",
    "\n",
    "val_dataset = PreprocessVOC2012(\n",
    "    val_img_list,  # イメージのパスリスト\n",
    "    val_anno_list, # アノテーションのパスリスト\n",
    "    phase=\"val\",   # 訓練モード\n",
    "    transform=DataTransform(input_size, color_mean), # 前処理オブジェクト\n",
    "    get_bbox_label=GetBBoxAndLabel(voc_classes),) # BBoxとラベル取得\n",
    "\n",
    "\n",
    "# 訓練用のミニバッチを生成するデータローダー\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,         # 前処理した訓練データ\n",
    "    batch_size=batch_size, # バッチサイズ\n",
    "    shuffle=True, # ミニバッチ抽出の際にシャッフルする\n",
    "    collate_fn=multiobject_collate_fn) # ミニバッチ生成関数\n",
    "\n",
    "# 検証用のミニバッチを生成するデータローダー\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,           # 前処理した検証データ\n",
    "    batch_size=batch_size, # バッチサイズ\n",
    "    shuffle=False, # ミニバッチ抽出の際にシャッフルしない\n",
    "    collate_fn=multiobject_collate_fn) # ミニバッチ生成関数\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8207d81-663a-441e-8e07-3596f9e11569",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab7e474-4415-4766-9fce-86721f780dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cv2.cvtColor(cv2.imread(val_img_list[4]), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56f77c55-e22f-4da8-98d5-8ad1ab9c51c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f023fe34-bcb5-4903-b707-d35173d81d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 3, 300, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuple[0]resizeされた画像\n",
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e83faf-2029-4779-be4f-99ed700f6dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 300, 300])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "027265fe-d475-476b-a416-5a12b9b781b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0900,  0.0300,  0.9980,  0.9970, 18.0000],\n",
       "        [ 0.1220,  0.5676,  0.1640,  0.7267, 14.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuple[1]はground truth\n",
    "# [cx, cy, w, h, クラス]\n",
    "# 例えば画像2枚目は2つの物体がある\n",
    "batch[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718edf0-41f8-418e-80f1-aaff263ae967",
   "metadata": {},
   "source": [
    "# モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c534e5e4-6b76-451e-b11d-366afe565490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Conv2d(512, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Conv2d(1024, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): Conv2d(512, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): Conv2d(256, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssd_cfg = {\n",
    "    'classes_num': 21,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 300,  # 画像の入力サイズ\n",
    "    'dbox_num': [4, 6, 6, 6, 4, 4],  # DBoxのアスペクト比の種類\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
    "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "# SSDモデルのvgg以外のネットワークの重みはHeの初期値で初期化\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight.data)\n",
    "        if m.bias is not None:  # バイアス項がある場合\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "net = SSD(\n",
    "    phase='train', # 訓練モード\n",
    "    cfg=ssd_cfg\n",
    ")   # 設定値のdictオブジェクト\n",
    "\n",
    "net.vgg.load_state_dict(torch.load(\"./data/vgg16_reducedfc.pth\"))\n",
    "\n",
    "# Heの初期値を適用\n",
    "net.extras.apply(weights_init) # extrasネットワーク\n",
    "net.loc.apply(weights_init)    # locネットワーク\n",
    "net.conf.apply(weights_init)   #confネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ad67826-2e82-4ebc-b6ad-9bdacf20cb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = net.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70186e04-8f8c-42e8-b7f2-76c3e55496d4",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "692526cf-ba2f-4510-9e5d-7e21ddf6e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数のオブジェクトを生成\n",
    "criterion = MultiBoxLoss(\n",
    "    jaccard_thresh=0.5, # 背景のDBoxに分類するときのIoUの閾値\n",
    "    neg_pos=3, # 背景のDBoxの数はPositive DBoxの何倍にするか\n",
    "    device=DEVICE) # ネットワークのTensorに割り当てるデバイス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f0e4f-c0fa-41b8-8a74-506af49a54f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Epoch 0/30\n",
      "---------------------------------------\n",
      "【epoch】:0, 【step】:0/119, loss: 26.9424\n",
      "【epoch】:0, 【step】:10/119, loss: 15.1216\n",
      "【epoch】:0, 【step】:20/119, loss: 12.1537\n",
      "【epoch】:0, 【step】:30/119, loss: 10.3944\n",
      "【epoch】:0, 【step】:40/119, loss: 8.5421\n",
      "【epoch】:0, 【step】:50/119, loss: 9.2846\n",
      "【epoch】:0, 【step】:60/119, loss: 8.7512\n",
      "【epoch】:0, 【step】:70/119, loss: 8.3279\n",
      "【epoch】:0, 【step】:80/119, loss: 8.2127\n",
      "【epoch】:0, 【step】:90/119, loss: 8.2097\n",
      "【epoch】:0, 【step】:100/119, loss: 8.9272\n",
      "【epoch】:0, 【step】:110/119, loss: 8.3313\n",
      "【epoch end】:0,【train loss】: 1222.4488, 【val loss】: 961.2\n",
      "---------------------------------------\n",
      "Epoch 1/30\n",
      "---------------------------------------\n",
      "【epoch】:1, 【step】:0/119, loss: 7.9264\n",
      "【epoch】:1, 【step】:10/119, loss: 7.9211\n",
      "【epoch】:1, 【step】:20/119, loss: 7.6182\n",
      "【epoch】:1, 【step】:30/119, loss: 7.9670\n",
      "【epoch】:1, 【step】:40/119, loss: 7.9442\n",
      "【epoch】:1, 【step】:50/119, loss: 8.1955\n",
      "【epoch】:1, 【step】:60/119, loss: 8.5226\n",
      "【epoch】:1, 【step】:70/119, loss: 8.0280\n",
      "【epoch】:1, 【step】:80/119, loss: 8.5442\n",
      "【epoch】:1, 【step】:90/119, loss: 8.0345\n",
      "【epoch】:1, 【step】:100/119, loss: 7.8233\n",
      "【epoch】:1, 【step】:110/119, loss: 7.6057\n",
      "【epoch end】:1,【train loss】: 974.0077, 【val loss】: 0.0\n",
      "---------------------------------------\n",
      "Epoch 2/30\n",
      "---------------------------------------\n",
      "【epoch】:2, 【step】:0/119, loss: 8.2050\n",
      "【epoch】:2, 【step】:10/119, loss: 8.5893\n",
      "【epoch】:2, 【step】:20/119, loss: 7.5095\n",
      "【epoch】:2, 【step】:30/119, loss: 7.5712\n",
      "【epoch】:2, 【step】:40/119, loss: 7.9221\n",
      "【epoch】:2, 【step】:50/119, loss: 7.7872\n",
      "【epoch】:2, 【step】:60/119, loss: 7.5410\n",
      "【epoch】:2, 【step】:70/119, loss: 7.2721\n",
      "【epoch】:2, 【step】:80/119, loss: 8.3514\n",
      "【epoch】:2, 【step】:90/119, loss: 7.0390\n",
      "【epoch】:2, 【step】:100/119, loss: 8.4788\n",
      "【epoch】:2, 【step】:110/119, loss: 7.2381\n",
      "【epoch end】:2,【train loss】: 945.9385, 【val loss】: 0.0\n",
      "---------------------------------------\n",
      "Epoch 3/30\n",
      "---------------------------------------\n",
      "【epoch】:3, 【step】:0/119, loss: 7.7590\n",
      "【epoch】:3, 【step】:10/119, loss: 7.5008\n",
      "【epoch】:3, 【step】:20/119, loss: 6.8378\n",
      "【epoch】:3, 【step】:30/119, loss: 6.6685\n",
      "【epoch】:3, 【step】:40/119, loss: 7.0038\n",
      "【epoch】:3, 【step】:50/119, loss: 7.0020\n",
      "【epoch】:3, 【step】:60/119, loss: 6.9892\n",
      "【epoch】:3, 【step】:70/119, loss: 7.0186\n",
      "【epoch】:3, 【step】:80/119, loss: 6.9053\n",
      "【epoch】:3, 【step】:90/119, loss: 8.1929\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "train_steps = len(dataloaders_dict[\"train\"].dataset)//batch_size\n",
    "\n",
    "# 勾配降下アルゴリズムを使用するオプティマイザーを生成\n",
    "optimizer = optim.SGD(\n",
    "    net.parameters(),  # SSDモデルのパラメーター\n",
    "    lr=1e-3,           # 学習率\n",
    "    momentum=0.9,      # 慣性項に割り当てる係数\n",
    "    weight_decay=5e-4) # 重み更新時のL2正則化の係数\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    print('---------------------------------------')\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "    # train\n",
    "    net.train()\n",
    "    epoch_train_loss = 0.\n",
    "    epoch_val_loss = 0.\n",
    "    for batch_idx, (images, targets) in enumerate(dataloaders_dict[\"train\"]):\n",
    "        images = images.to(DEVICE)\n",
    "        targets = [t.to(DEVICE) for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(images)\n",
    "        \n",
    "        loss_l, loos_c = criterion(outputs, targets)\n",
    "        loss = loss_l + loos_c\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_value_(net.parameters(), clip_value=2.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx == 0 or batch_idx % 10 == 0:\n",
    "            print(f\"【epoch】:{epoch}, 【step】:{batch_idx}/{train_steps}, loss: {loss.item():.4f}\")\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "    \n",
    "    # validation\n",
    "    net.eval()\n",
    "    if (epoch == 0) or epoch % 10 == 0:\n",
    "        for images, targets in dataloaders_dict[\"val\"]:\n",
    "            images = images.to(DEVICE)\n",
    "            targets = [t.to(DEVICE) for t in targets]\n",
    "            \n",
    "            outputs = net(images)\n",
    "            \n",
    "            loss_l, loos_c = criterion(outputs, targets)\n",
    "            loss = loss_l + loos_c\n",
    "            \n",
    "            epoch_val_loss += loss.item()\n",
    "            \n",
    "    \n",
    "    print(f\"【epoch end】:{epoch},【train loss】: {epoch_train_loss:.4f}, 【val loss】: {epoch_val_loss:.4}\")\n",
    "torch.save(net.state_dict(), \"./ssd_weight.ph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dea903-0e37-4596-a285-40e8e01b338f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
